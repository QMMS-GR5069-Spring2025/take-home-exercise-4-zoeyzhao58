{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e87eb2e-8497-4e2a-a3c9-3e2dbf577f93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load related dataset from S3\n",
    "df_pitstops = spark.read.csv('s3://columbia-gr5069-main/raw/pit_stops.csv', header=True)\n",
    "df_results = spark.read.csv('s3://columbia-gr5069-main/raw/results.csv', header=True)\n",
    "df_drivers = spark.read.csv('s3://columbia-gr5069-main/raw/drivers.csv', header=True)\n",
    "df_races = spark.read.csv('s3://columbia-gr5069-main/raw/races.csv', header=True)\n",
    "df_laptimes = spark.read.csv('s3://columbia-gr5069-main/raw/lap_times.csv', header=True)\n",
    "df_sprint_results = spark.read.csv('s3://columbia-gr5069-main/raw/sprint_results.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd53d6c4-fec8-436c-a9f7-673bff6423ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. [20 pts] Create two (2) new tables in your own fatabse where you'll store the predictions from each model for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfbf5734-bd0f-485d-9fee-ec675083ca7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create your own database and two result tables\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS my_f1_db\")\n",
    "# f1_model1_predictions: for classification predictions (Top 3 finish)\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS my_f1_db.f1_model1_predictions (\n",
    "    driverId INT,\n",
    "    raceId INT,\n",
    "    prediction INT\n",
    ")\n",
    "\"\"\")\n",
    "# f1_model2_predictions: for regression predictions (positionOrder)\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS my_f1_db.f1_model2_predictions (\n",
    "    driverId INT,\n",
    "    raceId INT,\n",
    "    prediction DOUBLE\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33ebdd2e-ad7c-4cbd-b985-13928f392ac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. [30 pts] Build two (2) predictive models using MLflow, logging hyperparameters, the model itself, four metrics, and two artifcats. Submit submit your MLflow experiments as part of your assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "216ccc39-8ba4-4525-a300-8575494dd51e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Model 1: Logistic Regression\n",
    "\n",
    "Goal: Predict whether a driver finishes in the Top 3 (positionOrder <= 3)\n",
    "\n",
    "Logged 4 metrics: AUC, Accuracy, F1, Precision\n",
    "\n",
    "Logged model + parameters\n",
    "\n",
    "Logged 2 artifacts: predictions CSV + coefficients TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b92210e-2e27-4e32-a382-92c7e2cd4f34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 16:29:10 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n2025/04/29 16:29:39 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/3911411105737321/28d8c7953f3540b4a0a9f353e620e057/artifacts/logistic_model/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback. \n/databricks/python/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Prepare data\n",
    "cls_df = df_results.select(\"driverId\", \"raceId\", \"grid\", \"laps\", \"fastestLap\", \"rank\", \"positionOrder\")\n",
    "cls_df = cls_df.withColumn(\"grid\", col(\"grid\").cast(\"int\")) \\\n",
    "               .withColumn(\"laps\", col(\"laps\").cast(\"int\")) \\\n",
    "               .withColumn(\"fastestLap\", col(\"fastestLap\").cast(\"int\")) \\\n",
    "               .withColumn(\"rank\", col(\"rank\").cast(\"int\")) \\\n",
    "               .withColumn(\"label\", (col(\"positionOrder\") <= 3).cast(\"int\")) \\\n",
    "               .na.drop()\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"grid\", \"laps\", \"fastestLap\", \"rank\"], outputCol=\"features\")\n",
    "cls_df = assembler.transform(cls_df)\n",
    "train_cls, test_cls = cls_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train and log with MLflow\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "with mlflow.start_run(run_name=\"F1_Logistic_Classifier\") as run:\n",
    "    model_cls = lr.fit(train_cls)\n",
    "    preds_cls = model_cls.transform(test_cls)\n",
    "\n",
    "    # Metrics\n",
    "    mlflow.log_metric(\"AUC\", BinaryClassificationEvaluator(labelCol=\"label\").evaluate(preds_cls))\n",
    "    mlflow.log_metric(\"accuracy\", MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\").evaluate(preds_cls))\n",
    "    mlflow.log_metric(\"f1\", MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\").evaluate(preds_cls))\n",
    "    mlflow.log_metric(\"precision\", MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"weightedPrecision\").evaluate(preds_cls))\n",
    "\n",
    "    # Params and model\n",
    "    mlflow.log_param(\"maxIter\", 10)\n",
    "    mlflow.spark.log_model(model_cls, \"logistic_model\")\n",
    "\n",
    "    # Artifacts\n",
    "    preds_cls.limit(50).toPandas().to_csv(\"/tmp/model1_preds.csv\", index=False)\n",
    "    mlflow.log_artifact(\"/tmp/model1_preds.csv\")\n",
    "\n",
    "    with open(\"/tmp/model1_coeffs.txt\", \"w\") as f:\n",
    "        f.write(str(model_cls.coefficients))\n",
    "    mlflow.log_artifact(\"/tmp/model1_coeffs.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "402133b9-31d0-4298-a270-54300db45127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Model 2: Linear Regression\n",
    "\n",
    "Goal: Predict a driver's finishing position (positionOrder)\n",
    "\n",
    "Logged 4 metrics: RMSE, MAE, MSE, RÂ²\n",
    "\n",
    "Logged model + parameters\n",
    "\n",
    "Logged 2 artifacts: predictions CSV + model summary TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "915b8933-43bd-43e4-a618-cb48b5afa7cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 16:34:46 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n2025/04/29 16:35:15 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/3911411105737321/52cc2aa81cc34ab7884afc653fcd98f1/artifacts/linear_model/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "reg_df = df_results.select(\"driverId\", \"raceId\", \"grid\", \"laps\", \"fastestLap\", \"rank\", \"positionOrder\")\n",
    "reg_df = reg_df.withColumn(\"grid\", col(\"grid\").cast(\"int\")) \\\n",
    "               .withColumn(\"laps\", col(\"laps\").cast(\"int\")) \\\n",
    "               .withColumn(\"fastestLap\", col(\"fastestLap\").cast(\"int\")) \\\n",
    "               .withColumn(\"rank\", col(\"rank\").cast(\"int\")) \\\n",
    "               .withColumn(\"label\", col(\"positionOrder\").cast(\"double\")) \\\n",
    "               .na.drop()\n",
    "\n",
    "reg_df = assembler.transform(reg_df)\n",
    "train_reg, test_reg = reg_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train and log with MLflow\n",
    "lr_reg = LinearRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "with mlflow.start_run(run_name=\"F1_Linear_Regression\") as run:\n",
    "    model_reg = lr_reg.fit(train_reg)\n",
    "    preds_reg = model_reg.transform(test_reg)\n",
    "\n",
    "    # Metrics\n",
    "    mlflow.log_metric(\"rmse\", RegressionEvaluator(labelCol=\"label\", metricName=\"rmse\").evaluate(preds_reg))\n",
    "    mlflow.log_metric(\"mae\", RegressionEvaluator(labelCol=\"label\", metricName=\"mae\").evaluate(preds_reg))\n",
    "    mlflow.log_metric(\"mse\", RegressionEvaluator(labelCol=\"label\", metricName=\"mse\").evaluate(preds_reg))\n",
    "    mlflow.log_metric(\"r2\", RegressionEvaluator(labelCol=\"label\", metricName=\"r2\").evaluate(preds_reg))\n",
    "\n",
    "    # Params and model\n",
    "    mlflow.log_param(\"maxIter\", 10)\n",
    "    mlflow.spark.log_model(model_reg, \"linear_model\")\n",
    "\n",
    "    # Artifacts\n",
    "    preds_reg.limit(50).toPandas().to_csv(\"/tmp/model2_preds.csv\", index=False)\n",
    "    mlflow.log_artifact(\"/tmp/model2_preds.csv\")\n",
    "\n",
    "    with open(\"/tmp/model2_summary.txt\", \"w\") as f:\n",
    "        f.write(str(model_reg.summary))\n",
    "    mlflow.log_artifact(\"/tmp/model2_summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17bc1f5c-313d-4393-a296-4d7605138be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. [30 pts] For each model, store its predictions in the corresponding table you created in your own database. Ensure you are using your own database to store your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd49d264-ef45-4553-94a4-cddbf5e2efa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean + Save Model 1 Predictions\n",
    "preds_cls.select(\n",
    "    col(\"driverId\").cast(\"int\").alias(\"driverId\"),\n",
    "    col(\"raceId\").cast(\"int\").alias(\"raceId\"),\n",
    "    col(\"prediction\").cast(\"int\").alias(\"prediction\")\n",
    ").write.mode(\"overwrite\").saveAsTable(\"my_f1_db.f1_model1_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27720dcf-e6ee-4f1c-8102-20a5527fac2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+\n|driverId|raceId|prediction|\n+--------+------+----------+\n|       1|  1000|         1|\n|       1|  1004|         1|\n|       1|  1006|         1|\n|       1|  1011|         1|\n|       1|  1017|         1|\n|       1|  1021|         1|\n|       1|  1027|         1|\n|       1|  1033|         1|\n|       1|  1043|         1|\n|       1|  1044|         0|\n|       1|  1045|         1|\n|       1|  1051|         1|\n|       1|  1053|         1|\n|       1|  1057|         1|\n|       1|  1065|         0|\n|       1|  1073|         1|\n|       1|  1081|         0|\n|       1|  1107|         0|\n|       1|    20|         0|\n|       1|    24|         1|\n+--------+------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM my_f1_db.f1_model1_predictions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2899960b-ce4f-4684-8f0f-efd7eea17fc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean + Save Model 2 Predictions\n",
    "preds_reg.select(\n",
    "    col(\"driverId\").cast(\"int\").alias(\"driverId\"),\n",
    "    col(\"raceId\").cast(\"int\").alias(\"raceId\"),\n",
    "    col(\"prediction\").cast(\"double\").alias(\"prediction\")\n",
    ").write.mode(\"overwrite\").saveAsTable(\"my_f1_db.f1_model2_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98712347-0f56-4c7b-a5ab-f947e40acfb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------------------+\n|driverId|raceId|        prediction|\n+--------+------+------------------+\n|       1|  1000| 3.318595383586363|\n|       1|  1004| 4.386778208298769|\n|       1|  1006| 3.151610982306848|\n|       1|  1011| 4.009033004853754|\n|       1|  1017|  4.15782079049644|\n|       1|  1021| 2.842635288355594|\n|       1|  1027|3.7832158723315636|\n|       1|  1033|1.9514811380677717|\n|       1|  1043|2.9287753412216526|\n|       1|  1044|  6.92082154071144|\n|       1|  1045|3.9843574839322624|\n|       1|  1051|3.6589522515996515|\n|       1|  1053|2.6189423616564076|\n|       1|  1057| 5.008795121108141|\n|       1|  1065|13.346934948265456|\n|       1|  1073| 4.206541294581313|\n|       1|  1081|   6.3018540733518|\n|       1|  1107| 7.015796979854864|\n|       1|    20|12.257026880719096|\n|       1|    24|7.8811548350880445|\n+--------+------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM my_f1_db.f1_model2_predictions\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "HW5_ZhuoyiZhao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}